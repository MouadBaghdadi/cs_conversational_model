# cs_conversational_model
I just tried to implement the seq2seq model architecture using 3 GRU layers with 256 units,
the model is trained on [QUESTION, ANSWER] data about the computer science field,
I used these hyper-parameters: 
  batch_size = 32
  emb_dim = 1024
  num_epochs = 40
  adam optimizer
